<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Engineering on LanceDB Blog</title><link>https://example.org/categories/engineering/</link><description>Recent content in Engineering on LanceDB Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 02 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/categories/engineering/index.xml" rel="self" type="application/rss+xml"/><item><title>Columnar File Readers in Depth: Repetition &amp; Definition Levels</title><link>https://example.org/blog/columnar-file-readers-in-depth-repetition-definition-levels/</link><pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/columnar-file-readers-in-depth-repetition-definition-levels/</guid><description>&lt;p>Repetition and definition levels are a method of converting structural arrays into a set of buffers. The approach was made popular in Parquet and is one of the key ways Parquet, ORC, and Arrow differ. In this blog I will explain how they work by contrasting them with validity &amp;amp; offsets buffers, and discuss how this encoding impacts I/O patterns.&lt;/p>





&lt;div class="admonition info">
 &lt;div class="admonition-content">
 &lt;div class="admonition-title">ðŸ“š Series Navigation&lt;/div>
 &lt;p>This is part of a series of posts on the details we&amp;rsquo;ve encountered building a columnar file reader:&lt;/p></description></item><item><title>Columnar File Readers in Depth: Column Shredding</title><link>https://example.org/blog/columnar-file-readers-in-depth-column-shredding/</link><pubDate>Thu, 15 May 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/columnar-file-readers-in-depth-column-shredding/</guid><description>&lt;p>Record shredding is a classic method used to transpose rows of potentially nested data into a flattened tree of buffers that can be written to the file. A similar technique, cascaded encoding, has recently emerged, that converts those arrays into a flattened tree of compressed buffers. In this article we explain and connect these ideas, but also explore the impacts of all this shredding on our goal of random access I/O, which leads to new approaches such as reverse shredding.&lt;/p></description></item><item><title>Columnar File Readers in Depth: Compression Transparency</title><link>https://example.org/blog/columnar-file-readers-in-depth-compression-transparency/</link><pubDate>Tue, 29 Apr 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/columnar-file-readers-in-depth-compression-transparency/</guid><description>&lt;p>Conventional wisdom states that compression and random access do not go well together. However, there are many ways you can compress data, and some of them support random access better than others. Figuring out which compression we can use, and when, and why, has been an interesting challenge. As we&amp;rsquo;ve been working on 2.1 we&amp;rsquo;ve developed a few terms to categorize different compression approaches.&lt;/p>





&lt;div class="admonition info">
 &lt;div class="admonition-content">
 &lt;div class="admonition-title">ðŸ“š Series Navigation&lt;/div>
 This is part of a series of posts on building a columnar file reader. The series starts &lt;a href="https://example.org/blog/columnar-file-readers-in-depth-apis-and-fusion/">here&lt;/a> and the most recent entry was &lt;a href="https://example.org/blog/columnar-file-readers-in-depth-backpressure/">here&lt;/a>.
 &lt;/div>
&lt;/div> 
&lt;h3 id="setting-the-stage">Setting the stage&lt;/h3>
&lt;p>Compression in Lance happens on large chunks of data for a single array. As batches of data come in, we split those batches into individual columns, and each column queues up data independently. Once a column&amp;rsquo;s queue is large enough, we trigger a flush for that column, and that initiates the compression process.&lt;/p></description></item><item><title>The Future of AI-Native Development is Local: Inside Continue's LanceDB-Powered Evolution</title><link>https://example.org/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/</link><pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/</guid><description>&lt;p>As Continue offers user-controlled IDE extensions, most of the codebase is written in TypeScript, and the data is stored locally in the &lt;code>~/.continue&lt;/code> folder. The tooling choices are made such that there are no separate processes required to handle database operations. Continue&amp;rsquo;s codebase retrieval features are powered by &lt;a href="https://github.com/lancedb/lancedb">LanceDB&lt;/a>, as it is the only vector database with an embedded TypeScript library that is capable of fast lookup times while being stored on disk and also supports SQL-like filtering.&lt;/p></description></item><item><title>A Practical Guide to Training Custom Rerankers</title><link>https://example.org/blog/a-practical-guide-to-training-custom-rerankers/</link><pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/a-practical-guide-to-training-custom-rerankers/</guid><description>&lt;h3 id="a-report-on-reranking-training--fine-tuning-rerankers-for-retrieval">A report on reranking, training, &amp;amp; fine-tuning rerankers for retrieval&lt;/h3>
&lt;hr>
&lt;p>This report offers practical insights for improving a retriever by reranking results. We&amp;rsquo;ll tackle the important questions, like: &lt;em>When should you implement a reranker? Should you opt for a pre-trained solution, fine-tune an existing model, or build one from scratch?&lt;/em>&lt;/p>
&lt;p>The retrieval process forms the backbone of RAG or agentic workflows. Chatbots and question-answer systems rely on retrievers to fetch context for every response during a conversation.
Most retrievers default to using vector search. It makes intuitive sense that improved embedding representations should yield better search results.&lt;/p></description></item><item><title>The Future of Open Source Table Formats: Apache Iceberg and Lance</title><link>https://example.org/blog/the-future-of-open-source-table-formats-iceberg-and-lance/</link><pubDate>Tue, 08 Apr 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/the-future-of-open-source-table-formats-iceberg-and-lance/</guid><description>&lt;p>As the scale of data continues to grow, open-source table formats have become essential for efficient data lake management. Apache Iceberg has emerged as a leader in this space, while new formats like &lt;a href="https://github.com/lancedb/lance">Lance&lt;/a> are introducing optimizations for specific workloads. In this post, we&amp;rsquo;ll explore how Iceberg and Lance address different challenges and complement each other in the evolving landscape of data lake table formats.&lt;/p>
&lt;h2 id="the-rise-of-open-source-table-formats">&lt;strong>The Rise of Open Source Table Formats&lt;/strong>&lt;/h2>
&lt;p>Table formats like Apache Iceberg, Delta Lake, and Apache Hudi were designed to address the challenges of managing large-scale structured data in cloud storage. These formats brought capabilities like ACID transactions, schema evolution, and time travel, making data lakes more reliable and performant. Among them, Iceberg gained significant traction through widespread adoption in the open-source ecosystem and strong integration into commercial vendor products. Its architecture enables scalable and performant query execution while allowing flexible integration into existing data lake infrastructure.&lt;/p></description></item><item><title>AnythingLLM's Competitive Edge: LanceDB for Seamless RAG and Agent Workflows</title><link>https://example.org/blog/anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows/</link><pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows/</guid><description>&lt;p>AnythingLLM chose LanceDB as their vector database backbone to create a frictionless experience for developers and end-users alike. By leveraging LanceDB&amp;rsquo;s serverless, setup-free architecture, the AnythingLLM team slashed engineering time previously spent on troubleshooting infrastructure issues and redirected it toward building innovative features. The result? An application that works seamlessly across all platforms with zero configuration or setup, empowering users to quickly deploy document chat and agentic workflows while maintaining complete data privacy and control.&lt;/p></description></item><item><title>Lance File 2.1: Smaller and Simpler</title><link>https://example.org/blog/lance-file-2-1-smaller-and-simpler/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/lance-file-2-1-smaller-and-simpler/</guid><description>&lt;p>Almost a year ago I announced we were going to be embarking on a journey to build a new 2.0 version of our file format. Several months later, we released a beta, and last fall it became our default file format. Overall, I&amp;rsquo;ve been super pleased with how well it worked. As we&amp;rsquo;ve been working with the community and stressing the format in production, we&amp;rsquo;ve identified a number of areas for improvement. We&amp;rsquo;ve been working on a 2.1 format for a few months to address these insights.&lt;/p></description></item><item><title>RAG with GRPO Fine-Tuned Reasoning Model</title><link>https://example.org/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/</guid><description>&lt;div class="admonition info">
 &lt;div class="admonition-content">
 &lt;div class="admonition-title">ðŸ’¡ Community Post&lt;/div>
 This is a community post by Mahesh Deshwal
 &lt;/div>
&lt;/div> 
&lt;p>&lt;strong>Group Relative Policy Optimization&lt;/strong> is the series of RL techniques for LLMs to guide them to specific goals. The process of creating a smart model these days is something like this:&lt;/p>
&lt;ol>
&lt;li>Pre Training a model on a HUGE corpus to get a pre-trained model&lt;/li>
&lt;li>Use the model created above on your data (usually Instructions or Que-Ans format) to fine-tune in an SFT (Supervised fine-tuning) manner.&lt;/li>
&lt;/ol>
&lt;p>Now let&amp;rsquo;s say a question can be answered in 100 different ways by the above model, all of them are correct. For example, the answer can be one word, sarcastic, politically incorrect, harmful, etc. If you have to &lt;strong>GUIDE&lt;/strong> the model to a specific set of rules for generation, you use the RL (Reinforcement Learning), where the popular techniques are PPO and DPO, and with DeepSeek, there comes GRPO.&lt;/p></description></item><item><title>Creating a FinTech AI Agent From Scratch</title><link>https://example.org/blog/creating-a-fintech-agent/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/creating-a-fintech-agent/</guid><description>&lt;p>AI agents are popping up everywhere, and they&amp;rsquo;re only going to become more common in the next few years. Think of them as a bunch of little digital assistantsâ€”agentsâ€”that keep an eye on things, make decisions, and get stuff done. It makes me wonder: how are we going to manage all these agents down the road? How do we trust them to handle decisions for us and let them run on their own?&lt;/p></description></item><item><title>End to End Evaluation Template for RAG Apps</title><link>https://example.org/blog/evaluate-rag-app-on-2/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/evaluate-rag-app-on-2/</guid><description>&lt;div class="admonition info">
 &lt;div class="admonition-content">
 &lt;div class="admonition-title">ðŸ’¡ Community Post&lt;/div>
 This is a community post by Mahesh Deshwal
 &lt;/div>
&lt;/div> 
&lt;h2 id="comprehensive-evaluation-metrics-for-rag-applications-using-lancedb">Comprehensive Evaluation Metrics for RAG Applications Using LanceDB&lt;/h2>
&lt;p>In the world of Retrieval-Augmented Generation (RAG) applications, evaluating the performance and reliability of models is critical. Evaluation metrics play a crucial role in assessing and enhancing the performance of models and their it is a continuous, iterative process.&lt;/p>
&lt;p>This involves two main types of evaluations: Offline and Online. Each type uses a variety of metrics to assess the system&amp;rsquo;s effectiveness and quality. In this blog post, we&amp;rsquo;ll delve deeply into these evaluation methods and metrics, with a particular focus on integrating LanceDB, a specialized vector database, to enhance performance.&lt;/p></description></item><item><title>Chunking Analysis: Which is the right chunking approach for your language?</title><link>https://example.org/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/</guid><description>&lt;p>Before we even start discussing the right chunking approach for your language, the first question that should come to mind is &lt;strong>whether the chunking approach depends on the language or not&lt;/strong>. Also, how much do you think the chunking method affects retrieval across languages when building RAG applications?&lt;/p>
&lt;p>You&amp;rsquo;ll find people using the same approach in all languages, and often it works, but is it the right approach? Do we have any analysis to back this approach? Have you ever thought of trying out new chunking methods, especially when working on a language-specific project? We&amp;rsquo;ll do that today. By the end of this blog, you&amp;rsquo;ll have answers to two questions:&lt;/p></description></item><item><title>Late Interaction &amp; Efficient Multi-modal Retrievers Need More Than a Vector Index</title><link>https://example.org/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/</guid><description>&lt;p>A lot has happened in the last few months in AI. Let&amp;rsquo;s look at what&amp;rsquo;s new with document retrieval.&lt;/p>
&lt;p>Try this through on Colab: &lt;a href="https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb">https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb&lt;/a>&lt;/p>
&lt;h2 id="late-interaction">Late Interaction&lt;/h2>
&lt;p>Retrieval models rely on embedding similarity between query and corpus(documents)&lt;/p>
&lt;p>&lt;img src="https://example.org/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-12-at-11.19.05-AM.png" alt="Retrieval Architecture">&lt;/p>
&lt;p>&lt;strong>Representation-focused rankers&lt;/strong> independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors. Think of cosine similarity on bi-encoder outputs of doc and query.&lt;/p></description></item><item><title>Lance v2: A New Columnar Container Format</title><link>https://example.org/blog/lance-v2/</link><pubDate>Sat, 13 Apr 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/lance-v2/</guid><description>&lt;h2 id="why-a-new-format">Why a New Format?&lt;/h2>
&lt;p>Lance was invented because readers and writers for existing column formats did not handle AI/ML workloads efficiently. Lance v1 solved some of these problems but still struggles in a number of cases. At the same time, others (&lt;a href="https://github.com/maxi-k/btrblocks">btrblocks&lt;/a>, &lt;a href="https://research.google/pubs/procella-unifying-serving-and-analytical-data-at-youtube/">procella&lt;/a>, &lt;a href="https://github.com/fulcrum-so/vortex">vortex&lt;/a>) have found similar issues with Parquet in their own use cases. I&amp;rsquo;d like to talk about a new format, Lance v2, that will solve these issues, but first let me describe the various use cases we have learned about by working with modern workloads.&lt;/p></description></item><item><title>A Practical Guide to Fine-Tuning Embedding Models</title><link>https://example.org/blog/a-practical-guide-to-fine-tuning-embedding-models/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/a-practical-guide-to-fine-tuning-embedding-models/</guid><description>&lt;p>This is a follow up to the following report that deals with improving retrievers by training and fine-tuning reranker models&lt;/p>
&lt;p>&lt;a href="https://example.org/a-practical-guide-to-training-custom-rerankers/">A Practical Guide to Training Custom Rerankers&lt;/a>&lt;/p>
&lt;p>In this report, we try to answer questions like - &lt;em>If/when should you fine-tune embedding models, and what are the qualities of a good fine-tuning dataset&lt;/em>&lt;/p>
&lt;p>We&amp;rsquo;ll deal with embedding part of the retrieval pipeline, which means any changes or updates will require re-ingestion of the data, unlike reranking.&lt;/p></description></item><item><title>Designing a Table Format for ML Workloads</title><link>https://example.org/blog/designing-a-table-format-for-ml-workloads/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/designing-a-table-format-for-ml-workloads/</guid><description>&lt;p>In recent years the concept of a &lt;strong>table format&lt;/strong> has really taken off, with explosive growth in technologies like &lt;a href="https://en.wikipedia.org/wiki/Apache_Iceberg">Iceberg&lt;/a>, Delta, and Hudi. With so many great options, one question I hear a lot is variations of &amp;ldquo;why can&amp;rsquo;t Lance use an existing format like &amp;hellip;?&amp;rdquo;&lt;/p>
&lt;p>In this blog post I will describe the Lance table format and hopefully answer that question. The very short TL;DR: &lt;strong>existing table formats don&amp;rsquo;t handle our customer&amp;rsquo;s workflows. Basic operations require too much data copy, are too slow, or cannot be parallelized.&lt;/strong>&lt;/p></description></item><item><title>GraphRAG: Hierarchical Approach to Retrieval-Augmented Generation</title><link>https://example.org/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/</guid><description>&lt;h2 id="what-is-rag">What is RAG?&lt;/h2>
&lt;p>&lt;strong>Retrieval-Augmented Generation (RAG)&lt;/strong> is an architecture that combines traditional information retrieval systems with large language models (LLMs). By integrating external knowledge sources, RAG enhances generative AI capabilities, allowing it to provide responses that are not only more accurate and relevant but also up to date.&lt;/p>
&lt;h2 id="how-does-retrieval-augmented-generation-rag-work">How Does Retrieval-Augmented Generation (RAG) Work?&lt;/h2>
&lt;p>RAG operates through a few key steps to enhance the performance of generative AI:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Retrieval and Pre-processing:&lt;/strong> RAG employs advanced search algorithms to access external data from sources such as websites, knowledge bases, and databases. The retrieved information is then pre-processed â€” cleaned, tokenized, and filtered â€” to ensure it&amp;rsquo;s ready for use.&lt;/li>
&lt;li>&lt;strong>Generation:&lt;/strong> The pre-processed data is integrated into the pre-trained LLM, enriching its context. This integration allows the LLM to generate responses that are more accurate, relevant, and informative.&lt;/li>
&lt;/ol>
&lt;p>These steps work together to make RAG a powerful tool for generating high-quality responses based on real-time information.&lt;/p></description></item></channel></rss>