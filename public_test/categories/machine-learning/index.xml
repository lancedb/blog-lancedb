<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on LanceDB Blog</title><link>https://example.org/categories/machine-learning/</link><description>Recent content in Machine Learning on LanceDB Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 27 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://example.org/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Creating a FinTech AI Agent From Scratch</title><link>https://example.org/blog/creating-a-fintech-agent/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://example.org/blog/creating-a-fintech-agent/</guid><description>&lt;p>AI agents are popping up everywhere, and they&amp;rsquo;re only going to become more common in the next few years. Think of them as a bunch of little digital assistants—agents—that keep an eye on things, make decisions, and get stuff done. It makes me wonder: how are we going to manage all these agents down the road? How do we trust them to handle decisions for us and let them run on their own?&lt;/p></description></item><item><title>Late Interaction &amp; Efficient Multi-modal Retrievers Need More Than a Vector Index</title><link>https://example.org/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/</guid><description>&lt;p>A lot has happened in the last few months in AI. Let&amp;rsquo;s look at what&amp;rsquo;s new with document retrieval.&lt;/p>
&lt;p>Try this through on Colab: &lt;a href="https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb">https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb&lt;/a>&lt;/p>
&lt;h2 id="late-interaction">Late Interaction&lt;/h2>
&lt;p>Retrieval models rely on embedding similarity between query and corpus(documents)&lt;/p>
&lt;p>&lt;img src="https://example.org/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-12-at-11.19.05-AM.png" alt="Retrieval Architecture">&lt;/p>
&lt;p>&lt;strong>Representation-focused rankers&lt;/strong> independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors. Think of cosine similarity on bi-encoder outputs of doc and query.&lt;/p></description></item><item><title>A Practical Guide to Fine-Tuning Embedding Models</title><link>https://example.org/blog/a-practical-guide-to-fine-tuning-embedding-models/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/a-practical-guide-to-fine-tuning-embedding-models/</guid><description>&lt;p>This is a follow up to the following report that deals with improving retrievers by training and fine-tuning reranker models&lt;/p>
&lt;p>&lt;a href="https://example.org/a-practical-guide-to-training-custom-rerankers/">A Practical Guide to Training Custom Rerankers&lt;/a>&lt;/p>
&lt;p>In this report, we try to answer questions like - &lt;em>If/when should you fine-tune embedding models, and what are the qualities of a good fine-tuning dataset&lt;/em>&lt;/p>
&lt;p>We&amp;rsquo;ll deal with embedding part of the retrieval pipeline, which means any changes or updates will require re-ingestion of the data, unlike reranking.&lt;/p></description></item><item><title>GraphRAG: Hierarchical Approach to Retrieval-Augmented Generation</title><link>https://example.org/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://example.org/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/</guid><description>&lt;h2 id="what-is-rag">What is RAG?&lt;/h2>
&lt;p>&lt;strong>Retrieval-Augmented Generation (RAG)&lt;/strong> is an architecture that combines traditional information retrieval systems with large language models (LLMs). By integrating external knowledge sources, RAG enhances generative AI capabilities, allowing it to provide responses that are not only more accurate and relevant but also up to date.&lt;/p>
&lt;h2 id="how-does-retrieval-augmented-generation-rag-work">How Does Retrieval-Augmented Generation (RAG) Work?&lt;/h2>
&lt;p>RAG operates through a few key steps to enhance the performance of generative AI:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Retrieval and Pre-processing:&lt;/strong> RAG employs advanced search algorithms to access external data from sources such as websites, knowledge bases, and databases. The retrieved information is then pre-processed — cleaned, tokenized, and filtered — to ensure it&amp;rsquo;s ready for use.&lt;/li>
&lt;li>&lt;strong>Generation:&lt;/strong> The pre-processed data is integrated into the pre-trained LLM, enriching its context. This integration allows the LLM to generate responses that are more accurate, relevant, and informative.&lt;/li>
&lt;/ol>
&lt;p>These steps work together to make RAG a powerful tool for generating high-quality responses based on real-time information.&lt;/p></description></item></channel></rss>