<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Late Interaction & Efficient Multi-modal Retrievers Need More Than a Vector Index - LanceDB Blog</title><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/syntax-highlighting.css><script src=/js/toc-progress.js></script><script src=/js/heading-links.js></script><script src=/js/announcement.js></script><script src=/js/code-blocks.js></script></head><body><div class=announcement-bar id=announcement-bar><div class=announcement-content><span>June 1st, 2025: LanceDB Cloud is now in public beta!</span>
<a href=https://lancedb.com/cloud class=announcement-link>Try it now →</a>
<button class=announcement-close onclick=closeAnnouncement()>×</button></div></div><header class=site-header><div class=header-content><a href=/ class=site-title><img src=/assets/blog/logo.png alt="LanceDB Blog" class=site-logo></a><div class=header-links><a href=/docs class=header-link>Documentation</a>
<a href=/pricing class=header-link>Pricing</a>
<a href=/get-started class="header-link get-started">Get Started</a></div></div></header><div class=content-wrapper><div class=toc-container><div class=toc><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#late-interaction>Late Interaction</a></li><li><a href=#late-interaction--vlms--vision-retriever>Late Interaction + VLMs = Vision Retriever!</a></li><li><a href=#lets-try-it-out>Let&rsquo;s Try It Out!</a><ul><li><a href=#need-for-an-efficient-retriever>Need for an Efficient Retriever</a></li><li><a href=#searching-for-relevant-docs>Searching for Relevant Docs</a></li></ul></li><li><a href=#retrieval>Retrieval</a><ul><li><a href=#limitations>Limitations</a></li><li><a href=#colpali-maxsims-as-a-reranking-step>ColPali MaxSims as a Reranking Step</a></li><li><a href=#other-strategies-for-reducing-the-search-space>Other Strategies for Reducing the Search Space</a></li><li><a href=#the-challenge-of-large-scale-vision-retrieval>The Challenge of Large-Scale Vision Retrieval</a></li><li><a href=#future-work>Future Work</a></li></ul></li></ul></nav></div></div><main><h1>Late Interaction & Efficient Multi-modal Retrievers Need More Than a Vector Index</h1><div class=post-meta><span class=post-category>Engineering</span>
<span class=meta-divider>•</span>
<span class=post-author>Ayush Chaurasia</span>
<span class=meta-divider>•</span>
<span class=post-date>September 18, 2024</span></div><article class=single-post><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index.jpg alt="Late Interaction & Efficient Multi-modal Retrievers Need More Than a Vector Index" class=preview-image><div class=post-content><p>A lot has happened in the last few months in AI. Let&rsquo;s look at what&rsquo;s new with document retrieval.</p><p>Try this through on Colab: <a href=https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb>https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb</a></p><h2 id=late-interaction>Late Interaction</h2><p>Retrieval models rely on embedding similarity between query and corpus(documents)</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-12-at-11.19.05-AM.png alt="Retrieval Architecture"></p><p><strong>Representation-focused rankers</strong> independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors. Think of cosine similarity on bi-encoder outputs of doc and query.</p><p><strong>Query-Document rankers</strong> Instead of summarizing q and d into individual embeddings, these rankers model word- and phrase-level relationships across q and d and match them using a deep neural network (e.g., with CNNs/MLPs). In the simplest case, they feed the neural network an interaction matrix that reflects the similarity between every pair of words across q and d.</p><p><strong>All-to-all Interaction rankers</strong> is a more powerful interaction-based paradigm, which models the interactions between words within as well as across q and d at the same time, as in BERT&rsquo;s transformer architecture. <strong>While interaction-based models tend to be superior for IR tasks, a representation-focused model—by isolating the computations among q and d—makes it possible to precompute document representations,</strong> greatly reducing the computational load per query.</p><p><strong>Late interaction rankers</strong> offer a new architecture where every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents.</p><p><strong>ColBert creates a vector index of all documents in the corpus offline, then computes the MaxSim during the query time.</strong></p><h2 id=late-interaction--vlms--vision-retriever>Late Interaction + VLMs = Vision Retriever!</h2><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/final_architecture.png alt="Final Architecture"></p><p>ColPali is a visual retriever model that combines the following:</p><ul><li>PaliGemma - A VLM combines <a href=https://huggingface.co/google/siglip-so400m-patch14-384><strong>SigLIP-So400m/14 vision encoder</strong></a> and <a href=https://unfoldai.com/gemma-2b/><strong>Gemma-2B</strong></a> language model. It also introduces projection layers to map language model inputs to 128-dim vectors</li><li>A late interaction mechanism based on ColBert</li></ul><p>Like ColBert, ColPali works in 2 phases:</p><p><strong>Offline:</strong></p><ul><li>Each document input is processed through the vision encoder and in patches. Each patch is then passed through the projection layer to get its vector representation.</li><li>Then these vectors are stored as multi-vector representations of the documents for retrieval at the query time</li><li>Each document is divided into 1030 patches with each patch resulting in a 128-dim vector.</li></ul><p><strong>Online:</strong></p><ul><li>At query time, the user input is encoded using a language model</li><li>Using the late interaction mechanism, MaxSims are calculated between query and already embedding document patches.</li><li>The similarity score of each patch is summer across the pages & the top K pages with maximum similarity score are returned as the final result</li></ul><h2 id=lets-try-it-out>Let&rsquo;s Try It Out!</h2><p>In this example, we&rsquo;ll make retrieval challenging by ingesting documents of very different genres:</p><ul><li><p>Investor relations/ Financial reports of Q2 2024 from:</p><ul><li>Apple</li><li>Amazon</li><li>Meta</li><li>Alphabet</li><li>Netflix</li><li>Starbucks</li></ul></li><li><p>Naturo Volume 72</p></li><li><p>Arabian Nights</p></li><li><p>Children&rsquo;s short story collection.</p></li><li><p>InraRed Cloud report</p></li><li><p>Short Stories for Children by Cuentos para la clase de Inglés</p></li></ul><p>You can also run it yourself in this colab walkthrough. The code is based on original work by <a href=https://x.com/truskovskiy>Kyryl Truskovskyi</a> which can be found <a href=https://kyrylai.com/2024/09/09/remove-complexity-from-your-rag-applications/>here</a></p><h3 id=need-for-an-efficient-retriever>Need for an Efficient Retriever</h3><p>In this pipeline, building an efficient retriever is of utmost importance as each query is compared to each document, which is stored in patches. Moreover, efficiently storing and retrieving the document images allows us to store the data and metadata in the same place to retrieve it when passing it on to the generator in case of a RAG setting. Efficient multi-modal data storage and retrieval is where LanceDB shines. It offers strong compute-storage separation, providing fast random access for retrieval while being persisted in storage.</p><p>The codebase is a derivative work on <a href=https://github.com/kyryl-opens-ml/vision-retrieval>vision-retriever</a>. It adds support for better ingestion through batch iterators which allows ingesting large datasets into Lance without running out of memory, meaning you can store your documents, vectors, and metadata in the same place without running OOM and also get fast retrieval and added features that we&rsquo;ll see next.</p><h3 id=searching-for-relevant-docs>Searching for Relevant Docs</h3><p>Let&rsquo;s take a look at some of the query and retrieved document pairs to get an idea of ColPali&rsquo;s performance.</p><p>Retrieving the docs from the table and then performing MaxSims operation on them with a query will look something like this with LanceDB:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>colpali_engine.trainer.retrieval_evaluator</span> <span class=kn>import</span> <span class=n>CustomEvaluator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>search</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>processor</span><span class=p>,</span> <span class=n>top_k</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>qs</span> <span class=o>=</span> <span class=n>get_query_embedding</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>query</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>processor</span><span class=o>=</span><span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Retrieve all documents</span>
</span></span><span class=line><span class=cl>    <span class=n>r</span> <span class=o>=</span> <span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>()</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=kc>None</span><span class=p>)</span><span class=o>.</span><span class=n>to_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>process_patch_embeddings</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>patches</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=s1>&#39;page_embedding_flatten&#39;</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=s1>&#39;page_embedding_shape&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>patches</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>all_pages_embeddings</span> <span class=o>=</span> <span class=p>[</span><span class=n>process_patch_embeddings</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>r</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>retriever_evaluator</span> <span class=o>=</span> <span class=n>CustomEvaluator</span><span class=p>(</span><span class=n>is_multi_vector</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>retriever_evaluator</span><span class=o>.</span><span class=n>evaluate_colbert</span><span class=p>([</span><span class=n>qs</span><span class=p>[</span><span class=s2>&#34;embeddings&#34;</span><span class=p>]],</span> <span class=n>all_pages_embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>top_k_indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=n>top_k</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>indices</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>top_k_indices</span><span class=p>[</span><span class=mi>0</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=n>page</span> <span class=o>=</span> <span class=n>r</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>pil_image</span> <span class=o>=</span> <span class=n>base64_to_pil</span><span class=p>(</span><span class=n>page</span><span class=p>[</span><span class=s2>&#34;image&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=n>page</span><span class=p>[</span><span class=s2>&#34;name&#34;</span><span class=p>],</span> <span class=s2>&#34;page_idx&#34;</span><span class=p>:</span> <span class=n>page</span><span class=p>[</span><span class=s2>&#34;page_idx&#34;</span><span class=p>],</span> <span class=s2>&#34;pil_image&#34;</span><span class=p>:</span> <span class=n>pil_image</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>results</span>
</span></span></code></pre></div><p>The device used for this experiment:</p><ul><li>GPU - Nvidia V100 16GB</li><li>RAM - 30GB</li></ul><p>Total documents pages ingested - 556</p><h2 id=retrieval>Retrieval</h2><p>In this section, we&rsquo;ll see:</p><ul><li>Performance of ColPali in retrieving the correct document. In this case, it&rsquo;ll use MaxSim operation across ALL ingested document pages.</li><li>Optimizing lookup time by reducing search space using LanceDB FTS (<strong>ColPali as a FTS reranker</strong>)</li><li>Optimizing lookup time by reducing search space using LanceDB Semantic search (<strong>ColPali as a vector search reranker</strong>)</li></ul><ol><li>First, we&rsquo;ll ask a question about the trends in model training costs. This should be covered in the infraRed cloud report doc.</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;How do model training costs change over time?&#34;</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>search</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>search_result</span> <span class=o>=</span> <span class=n>search</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>query</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>colpali</span><span class=p>,</span> <span class=n>processor</span><span class=o>=</span><span class=n>processor</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>search_result</span><span class=p>[</span><span class=s2>&#34;image&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>Time taken - 34 seconds</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-15-at-10.39.27-PM.png alt="Training Costs Result"></p><p>It got the exact doc that describes the training cost trend as a graph. This is especially interesting because we&rsquo;ve ingested many documents from similar genres (financial reports of big tech and other organizations). Any decent VLM can interpret this to form a final response if it&rsquo;s an RAG setting.</p><p>While late interaction with ColPali allows you to compute the document patch-embedding offline ahead of query time, it still needs to perform MaxSim operation to calculate the <strong>similarity between a query and ALL documents, which is an expensive operation.</strong> As the number of documents grows, the total time taken will also increase proportionally. Of course, using a more powerful GPU will result in better performance but it&rsquo;ll still be relative.</p><p>Let&rsquo;s take a look at another example and then move on to some optimizations that can allow this workflow to be used in a real-world setting.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;What did the queen ask her magic mirror everyday?&#34;</span>
</span></span></code></pre></div><p>Time taken - 30.50 seconds</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-12.32.11-AM.png alt="Magic Mirror Result"></p><p>Again, it got to the exact page where the evil queen from snow white asks the given question to her magic mirror.</p><h3 id=limitations>Limitations</h3><p>Using ColPali as a 1-step vision retriever greatly simplifies the process of document retrieval but without any additional optimizations, as the document index grows, the retrieval operation at query time will keep getting expensive. Let us now look at some ways to speed up retrieval by reducing the search space.</p><h3 id=colpali-maxsims-as-a-reranking-step>ColPali MaxSims as a Reranking Step</h3><p>There are a couple of hacks that can be used to reduce the search space by filtering out some results.</p><ol><li><strong>If text field is available</strong></li></ol><p>In case you&rsquo;re able to extract the text from the pdf (Here we&rsquo;re not referring to OCR. Some pdf encodings allow reading text directly from the file), you can create an FTS index on the text column to try and reduce the search space. This will effectively make the ColPali MaxSims operation a reranking step for FTS. In this example, we&rsquo;ll get the top 100 FTS matches, which brings down the search space to about 1/5th.</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-17-at-5.28.22-PM.png alt="FTS Optimization"></p><p>This is how the implementation would look like:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>search</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>table_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>processor</span><span class=p>,</span> <span class=n>db_path</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;lancedb&#34;</span><span class=p>,</span> <span class=n>top_k</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span> <span class=n>fts</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>limit</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>qs</span> <span class=o>=</span> <span class=n>get_query_embedding</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>query</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>processor</span><span class=o>=</span><span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># Search over all dataset </span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>fts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># set 100 as the max limit if filtering / reducing </span>
</span></span><span class=line><span class=cl>        <span class=c1># search space</span>
</span></span><span class=line><span class=cl>        <span class=n>limit</span> <span class=o>=</span> <span class=n>limit</span> <span class=ow>or</span> <span class=mi>100</span> 
</span></span><span class=line><span class=cl>        <span class=n>r</span> <span class=o>=</span> <span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>query_type</span><span class=o>=</span><span class=s2>&#34;fts&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=n>limit</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>r</span> <span class=o>=</span> <span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>()</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=n>limit</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>r</span> <span class=o>=</span> <span class=n>r</span><span class=o>.</span><span class=n>to_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    <span class=c1># Same as described above</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>results</span>
</span></span></code></pre></div><p>Let&rsquo;s take a look at the result of the 2nd query.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;What did the queen ask her magic mirror everyday?&#34;</span>
</span></span></code></pre></div><p>Time taken - 6.30 seconds</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-12.32.11-AM-2.png alt="Magic Mirror FTS Result"></p><p>We get the expected result with the latency reduced to about 1/5th.</p><ol start=2><li><strong>Reducing the search space using a similarity search</strong></li></ol><p>A vision retriever pipeline ideally shouldn&rsquo;t be dependent on being able to parse the text from the documents as it defeats the purpose of being a 1-shot retrieval method.</p><p><strong>Hypothesis</strong></p><p>Remember, 128 dim vector projections are derived from the language model part of ColPali. Although query dim (in this case 25x128) isn&rsquo;t the same as the doc path embedding dim (1030x128). However because these are the representations of the same model, they might be able to capture the similarity between the query and the doc patches. So we can flatten them out, zero pad the query embeddings to match the doc patch embeddings, and run vector search to filter out the top_k and reduce the search space for ColPali MaxSims reranking.</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-17-at-5.36.44-PM.png alt="Vector Search Optimization"></p><p>This&rsquo;ll look something like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>search</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>processor</span><span class=p>,</span> <span class=n>top_k</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span> <span class=n>fts</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>vector</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>limit</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>qs</span> <span class=o>=</span> <span class=n>get_query_embedding</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>query</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span> <span class=n>processor</span><span class=o>=</span><span class=n>processor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Search over all dataset</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>vector</span> <span class=ow>and</span> <span class=n>fts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&#34;can&#39;t filter using both fts and vector&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>fts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>limit</span> <span class=o>=</span> <span class=n>limit</span> <span class=ow>or</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>        <span class=n>r</span> <span class=o>=</span> <span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>query_type</span><span class=o>=</span><span class=s2>&#34;fts&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=n>limit</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>vector</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>limit</span> <span class=o>=</span> <span class=n>limit</span> <span class=ow>or</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>        <span class=n>vec_q</span> <span class=o>=</span> <span class=n>flatten_and_zero_pad</span><span class=p>(</span><span class=n>qs</span><span class=p>[</span><span class=s2>&#34;embeddings&#34;</span><span class=p>],</span><span class=n>table</span><span class=o>.</span><span class=n>to_pandas</span><span class=p>()[</span><span class=s2>&#34;page_embedding_flatten&#34;</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>r</span> <span class=o>=</span> <span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>vec_q</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>(),</span> <span class=n>query_type</span><span class=o>=</span><span class=s2>&#34;vector&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=n>limit</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>r</span> <span class=o>=</span> <span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>()</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=n>limit</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>where</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>r</span> <span class=o>=</span> <span class=n>r</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>where</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>r</span> <span class=o>=</span> <span class=n>r</span><span class=o>.</span><span class=n>to_list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    <span class=c1># Same as before</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>results</span>
</span></span></code></pre></div><p><strong>Results:</strong></p><p>Let&rsquo;s try the previous query and then a new one:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;What did the queen ask her magic mirror everyday?&#34;</span>
</span></span></code></pre></div><p>Time taken = ~6 seconds</p><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-12.32.11-AM-2-2.png alt="Magic Mirror Vector Result"></p><p>Let&rsquo;s run a couple of new tests and also verify the results are the same with and without pre-filtering with vector search.</p><p>Now, let&rsquo;s a question from one of the stories from the Arabian Nights, where a lady scatters some water around in a lake and the fishes turn into humans. Let&rsquo;s see if it can find that document.</p><p>Time taken ~ 6 seconds</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;How did the fish become men, women, and children&#34;</span>
</span></span></code></pre></div><p><img src=/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-3.39.06-PM.png alt="Arabian Nights Result"></p><p>It does find the exact document!</p><h3 id=other-strategies-for-reducing-the-search-space>Other Strategies for Reducing the Search Space</h3><p>There are more obvious ways of reducing the search space with some prior information. For example, if you already know the document from which the query is requested, you can simply use filters.</p><p>Also, with a little bit of more processing on the offline indexing side, you classify the document genre and add that as another property when ingesting the docs.</p><h3 id=the-challenge-of-large-scale-vision-retrieval>The Challenge of Large-Scale Vision Retrieval</h3><p>ColPali is definitely a step function change in the document retrieval process. It greatly simplifies the process of indexing documents. OCR is a complex modeling challenge and can be a point of failure on its own. Chunking and embedding the texts from the OCR model again is another hyper-parameter to take care of. There are <a href=https://github.com/Layout-Parser/layout-parser>dedicated tools</a> to analyze the visual layout of the contents of docs for better retrieval. ColPali as a vision retriever allows the elimination of these individual, error-prone methods while still being relatively efficient.</p><p>But making vision retrieval work on a large scale requires more than just a traditional vector index. Because of the high dimensionality of patch embeddings (1030x128), creating a vector index across a large-scale dataset is not feasible for an in-memory DB. Moreover, reducing search space by using BM25/FTS or efficient filtering, storing and retrieving actual image data is also required.</p><p><strong>LanceDB shines when it comes to large-scale multi-modal applications</strong>. LanceDB is fundamentally different from other vector databases in that it is built on top of <a href=https://github.com/lancedb/lance>Lance</a>, an open-source columnar data format designed for performant ML workloads and fast random access.</p><ul><li>Due to the design of Lance, LanceDB&rsquo;s indexing philosophy adopts a primarily <em>disk-based</em> indexing philosophy. It can also simply store and retrieve objects so you can have your vectors, metadata, as well <strong>patch-level</strong> dataset files in the same place.</li><li>Comes with native support for FTS and semantic search which allows efficient retrieval for multi-modal datasets as we&rsquo;ve seen in this blog.</li></ul><h3 id=future-work>Future Work</h3><p>Vision retrieval with VLMs is a new retrieval technique and experiment results still pouring in. Multi-vector patch-level embeddings can capture document visual layout and semantic meaning of the text but indexing large vectors can be an expensive operations. In this blog, we&rsquo;ve seen some methods of reducing the search space to speed up the retrieval process. Some other ways to optimize this would be the binarization of document embeddings to reduce the dimensionality of the vectors by 26 to 32 times for efficient storage and faster search, as covered in this detailed blog post by Jo from Vespa - <a href=https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/>https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/</a></p><p>Try out this example - <a href=https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb>https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb</a></p><p>Or others on <a href=https://github.com/lancedb/vectordb-recipes>vectorDB recipes</a>.</p></div></article><div class=author-section><img src=/assets/authors/ayush-chaurasia.jpg alt="Ayush Chaurasia" class=author-avatar><div class=author-info><h3 class=author-name>Ayush Chaurasia</h3><p class=author-bio>ML Engineer and researcher focused on multi-modal AI systems and efficient retrieval methods.</p><div class=author-social><a href=https://twitter.com/ayushchaurasia target=_blank rel=noopener><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
Twitter
</a><a href=https://github.com/ayushchaurasia target=_blank rel=noopener><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
GitHub
</a><a href=https://linkedin.com/in/ayushchaurasia target=_blank rel=noopener><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
LinkedIn</a></div></div></div><div class=related-posts><h2>Related Posts</h2><div class=related-posts-grid><article class=related-post><a href=/blog/columnar-file-readers-in-depth-repetition-definition-levels/><img src=/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/columnar-file-readers-in-depth-repetition-definition-levels.png alt="Columnar File Readers in Depth: Repetition & Definition Levels" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-repetition-definition-levels/>Columnar File Readers in Depth: Repetition & Definition Levels</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>June 2, 2025</span></div></article><article class=related-post><a href=/blog/columnar-file-readers-in-depth-column-shredding/><img src=/assets/blog/columnar-file-readers-in-depth-column-shredding/columnar-file-readers-in-depth-column-shredding.png alt="Columnar File Readers in Depth: Column Shredding" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-column-shredding/>Columnar File Readers in Depth: Column Shredding</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>May 15, 2025</span></div></article><article class=related-post><a href=/blog/columnar-file-readers-in-depth-compression-transparency/><img src=/assets/blog/columnar-file-readers-in-depth-compression-transparency/columnar-file-readers-in-depth-compression-transparency.png alt="Columnar File Readers in Depth: Compression Transparency" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-compression-transparency/>Columnar File Readers in Depth: Compression Transparency</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>April 29, 2025</span></div></article></div></div></main></div><footer class=site-footer><div class=footer-content><a href=/ class=site-title><img src=/assets/blog/logo.png alt="LanceDB Blog" class=site-logo></a><div class=footer-links><a href=https://lancedb.com/documentation class=footer-link>Documentation</a>
<a href=https://lancedb.com/pricing class=footer-link>Pricing</a>
<a href=/get-started class="footer-link get-started">Get Started</a></div></div></footer></body></html>