<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Customer Support Bot : RASA x LanceDB - LanceDB Blog</title><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/syntax-highlighting.css><script src=/js/toc-progress.js></script><script src=/js/heading-links.js></script><script src=/js/announcement.js></script><script src=/js/code-blocks.js></script></head><body><div class=announcement-bar id=announcement-bar><div class=announcement-content><span>June 1st, 2025: LanceDB Cloud is now in public beta!</span>
<a href=https://lancedb.com/cloud class=announcement-link>Try it now →</a>
<button class=announcement-close onclick=closeAnnouncement()>×</button></div></div><header class=site-header><div class=header-content><a href=/ class=site-title><img src=/assets/blog/logo.png alt="LanceDB Blog" class=site-logo></a><div class=header-links><a href=/docs class=header-link>Documentation</a>
<a href=/pricing class=header-link>Pricing</a>
<a href=/get-started class="header-link get-started">Get Started</a></div></div></header><div class=content-wrapper><div class=toc-container><div class=toc><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#what-is-rasa>What is RASA?</a></li><li><a href=#quick-overview>Quick Overview</a></li></ul><ul><li><ul><li></li></ul></li></ul><ul><li><ul><li></li><li><a href=#step-6---interact-with-the-bot-and-ask-it-questions>Step 6 - Interact with the bot and ask it questions</a></li></ul></li><li><a href=#colab-walkthrough>Colab Walkthrough</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div><main><h1>Customer Support Bot : RASA x LanceDB</h1><div class=post-meta><span class=post-author>Rithik Kumar</span>
<span class=meta-divider>•</span>
<span class=post-date>December 31, 2024</span></div><article class=single-post><img src=/assets/blog/customer-support-bot-rasa-x-lancedb/customer-support-bot-rasa-x-lancedb.png alt="Customer Support Bot : RASA x LanceDB" class=preview-image><div class=post-content><p>Have you ever wondered how businesses manage to provide instant, accurate, and personalized customer support around the clock? This article will teach us how to make an <strong>Advanced Customer Support Chatbot</strong> using <strong>Rasa</strong>, <strong>LanceDB</strong>, and <strong>OpenAI&rsquo;s Large Language Models (LLMs)</strong>.</p><h2 id=what-is-rasa>What is RASA?</h2><p><strong>Rasa</strong> is an open-source framework designed to build intelligent, contextual, and scalable chatbots and virtual assistants. Unlike some one-size-fits-all solutions, Rasa offers the flexibility to customize your bot&rsquo;s behavior, making it perfectly tailored to your business needs.</p><ol><li><strong>Open-Source Goodness:</strong> Free to use and highly customizable.</li><li><strong>Natural Language Understanding (NLU):</strong> Rasa interprets user inputs to identify intents and extract relevant entities, enabling the chatbot to understand the purpose behind each query.</li><li><strong>Dialogue Management:</strong> Rasa manages the flow of conversation, maintaining context across multiple interactions and ensuring coherent and context-aware responses.</li><li><strong>Custom Actions:</strong> Through its <code>actions.py</code> file, Rasa executes custom actions that perform specific tasks, such as connecting with databases, APIs, and other services like LanceDB and OpenAI&rsquo;s LLMs.</li></ol><h2 id=quick-overview>Quick Overview</h2><p>This guide explains the process of building an <strong>Advanced Customer Support Chatbot</strong> by integrating <strong>Rasa</strong>, a robust conversational framework; <strong>LanceDB</strong>, a high-performance vector database; and <strong>OpenAI&rsquo;s LLM</strong>, a state-of-the-art language model.
<img src=__GHOST_URL__/content/images/2024/12/NLU--2--1.jpg alt>How RASA performs custom actions to do the query on Lance Table and then perform API call to LLM to generate refined response
So, you&rsquo;ve got Rasa, LanceDB, and OpenAI LLMs ready to join forces. How do these components work together to create a seamless customer support chatbot? Let&rsquo;s break it down:</p><ol><li><strong>User Interaction:</strong></li></ol><ul><li>A customer sends a query to the chatbot, such as <em>&ldquo;How do I reset my password?&rdquo;</em></li></ul><ol start=2><li><strong>Rasa NLU and Core:</strong></li></ol><ul><li>Rasa NLU interprets the intent and extracts relevant entities (if any) and then Rasa core decides if it&rsquo;s time to trigger custom action or give direct response.</li></ul><ol start=3><li><strong>Fetching Knowledge from LanceDB:</strong></li></ol><ul><li>Using the intent and entities, Rasa triggers a custom action that queries LanceDB for specific support information related to user query (password resets).</li></ul><ol start=4><li><strong>Generating a Smart Response with OpenAI:</strong></li></ol><ul><li>The retrieved information from LanceDB is then fed into OpenAI&rsquo;s LLM, which crafts a comprehensive and personalized response for the customer.</li></ul><ol start=5><li><strong>Delivering the Response:</strong></li></ol><ul><li>The chatbot sends the generated response back to the customer, ensuring they receive clear and helpful guidance.</li></ul><p>This integration ensures that your chatbot is not only responsive but also intelligent, capable of handling a wide array of customer inquiries with ease.</p><h1 id=lets-begin>Let&rsquo;s begin</h1><p>Now we are clear about the flow of this project, let&rsquo;s delve into coding it. Make sure you have OPENAI_API key for this project.</p><h4 id=install-dependencies>Install dependencies</h4><pre><code>pip install rasa lancedb openai==0.28 python-dotenv -q
</code></pre><p>If installing on colab you will need to restart the session after installing the packages - <em><strong>ctrl + M.</strong></em></p><h4 id=initialize-a-new-rasa-project-which-sets-up-the-necessary-directory-structure-and-files>Initialize a new Rasa project which sets up the necessary directory structure and files.</h4><pre><code>rasa init --no-prompt
</code></pre><p>This command sets up the basic directory structure with sample data. To verify the setup, you can train the initial model and interact with the sample bot:</p><pre><code>rasa train
rasa shell
</code></pre><h4 id=create-a-env-file-and-store-necessary-environment-variables-in-it>Create a .env file and store necessary environment variables in it</h4><pre><code>OPENAI_API_KEY = &quot;sk-*********&quot;
</code></pre><h4 id=step-1---embed-all-the-customer-support-data-in-lancedb-which-can-be-queried-by-rasa-custom-actions-later>Step 1 - Embed all the customer support data in LanceDB which can be queried by RASA custom actions later</h4><p><strong>LanceDB</strong> serves as the knowledge base for the chatbot, storing FAQs and support information. The following steps outline the setup process:</p><pre><code># Import necessary libraries
import os
import subprocess
import time
import threading
import lancedb
import requests
import json
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry

# Initialize LanceDB
db = lancedb.connect(&quot;./content/lancedb&quot;)  # Local storage within Colab

# Initialize the language model for generating embeddings
model = get_registry().get(&quot;sentence-transformers&quot;).create(name=&quot;BAAI/bge-small-en-v1.5&quot;, device=&quot;cpu&quot;)

# Create table from schema
class Documents(LanceModel):
    vector: Vector(model.ndims()) = model.VectorField()
    content: str = model.SourceField() # Field to store the actual content/response

company_support_data = [
    { &quot;content&quot;: &quot;To reset your password, navigate to the login page and click on 'Forgot Password'. You'll receive an email with instructions to create a new password.&quot; },
    { &quot;content&quot;: &quot;To update your account information, log in to your profile and click on 'Edit Profile'. From there, you can change your email, phone number, and other personal details.&quot; }
    #,.... rest of the data
]
    
# Knowledge data from the data above
knowledge_data = company_support_data

# Define table name
table_name = &quot;knowledge_base&quot;

# Retrieve existing table names
existing_tables = db.table_names()

if table_name not in existing_tables:
    # Create a new table with the schema and insert data
    tbl = db.create_table(table_name, schema=Documents)
    tbl.add(knowledge_data)
    print(f&quot;Created new table '{table_name}' and inserted data.&quot;)
else:
    # Append data to the existing table
    table = db.open_table(table_name)
    table.add(knowledge_data, mode=&quot;overwrite&quot;)
    print(f&quot;Overwrited data to the existing table '{table_name}'.&quot;)
</code></pre><p>We will be using pre-trained <strong>Sentence Transformer model</strong> (BAAI/bge-small-en-v1.5) to convert textual support data into vector embeddings. You can change it accordingly.</p><p>The code above will create a table - &ldquo;knowledge_base&rdquo; in the DB and insert all the customer support information in this table.</p><h4 id=step-2---configure-rasa-files-according-to-our-use-case>Step 2 - Configure RASA files according to our use case</h4><ul><li><p><strong>domain.yml</strong> - The domain.yml file serves as the core configuration for your Rasa chatbot. It defines the chatbot&rsquo;s intents, entities, slots, responses, actions, forms, and policies.</p><p>version: &ldquo;3.0&rdquo;
language: &ldquo;en&rdquo;
intents:
- greet
- ask_knowledge
- goodbye</p><p>entities:
- project
- service
responses:
utter_greet:
- text: &ldquo;Hello! How can I assist you today?&rdquo;
utter_goodbye:
- text: &ldquo;Goodbye! Have a great day!&rdquo;
- text: &ldquo;Bye! Let me know if you need anything else.&rdquo;
- text: &ldquo;See you later! Feel free to reach out anytime.&rdquo;
actions:
- action_search_knowledge</p></li><li><p><strong>endpoints.yml</strong> - It specifies the URLs and connection details for services like the custom action server (actions.py), enabling Rasa to communicate with it.</p><p>action_endpoint:
url: &ldquo;http://localhost:5055/webhook&rdquo;</p></li><li><p><strong>data/stories.yml</strong> - The stories.yml file contains training stories that define example conversational paths your chatbot can take.</p><p>version: &ldquo;3.0&rdquo;
stories:
- story: Greet and ask question
steps:
- intent: greet
- action: utter_greet
- intent: ask_knowledge
- action: action_search_knowledge</p><pre><code>- story: ask question
  steps:
    - intent: ask_knowledge
    - action: action_search_knowledge

- story: Goodbye
  steps:
    - intent: goodbye
    - action: utter_goodbye

- story: greet and goodbye
  steps:
    - intent: greet
    - action: utter_greet
    - intent: goodbye
    - action: utter_goodbye
</code></pre></li><li><p>**data/rules.yml **- The rules.yml file defines rule-based conversations that specify exact steps the chatbot should follow in certain situations. Unlike stories, rules are strict paths that Rasa should follow without deviation.</p><p>version: &ldquo;3.0&rdquo;
rules:
- rule: Greet
steps:
- intent: greet
- action: utter_greet</p><pre><code>- rule: Goodbye
  steps:
    - intent: goodbye
    - action: utter_goodbye

- rule: Answer Knowledge Questions
  steps:
    - intent: ask_knowledge
    - action: action_search_knowledge
</code></pre></li><li><p><strong>data/nlu.yml</strong> - The nlu.yml file contains Natural Language Understanding (NLU) training data. It includes examples of user inputs categorized by intents and annotated with entities to train Rasa&rsquo;s NLU component.</p><p>version: &ldquo;3.0&rdquo;
nlu:
- intent: greet
examples: |
- hello
- hi
- hey
- good morning
- good evening
- greetings</p><pre><code>- intent: goodbye
  examples: |
    - bye
    - goodbye
    - see you later
    - catch you later
    - see ya
    - take care

- intent: ask_knowledge
  examples: |
    - I need help with my account
    - Can you assist me with billing?
    - How do I reset my password?
    - I'm facing issues with my order
    - Tell me about your support services
    - How can I contact customer service?
    - What are your support hours?
    - I have a question about Project Alpha
    - Help me understand Project Beta
    - How can I track my purchase?
</code></pre></li><li><p><strong>config.yml</strong> - The config.yml file defines the pipeline and policies used by Rasa for processing natural language inputs and managing dialogue workflows.</p><p>version: &ldquo;3.0&rdquo;
language: &ldquo;en&rdquo;
pipeline:</p><ul><li>name: WhitespaceTokenizer</li><li>name: RegexFeaturizer</li><li>name: LexicalSyntacticFeaturizer</li><li>name: CountVectorsFeaturizer</li><li>name: CountVectorsFeaturizer
analyzer: char_wb
min_ngram: 1
max_ngram: 4</li><li>name: DIETClassifier
epochs: 100</li><li>name: EntitySynonymMapper</li><li>name: ResponseSelector
epochs: 100</li></ul><p>policies:</p><ul><li>name: RulePolicy</li><li>name: UnexpecTEDIntentPolicy
max_history: 5
epochs: 100</li><li>name: TEDPolicy
max_history: 5
epochs: 100
assistant_id: 20241227-151505-young-attachment</li></ul></li></ul><h4 id=step-3---implement-custom-actions-actionspy-file>Step 3 - Implement Custom Actions (actions.py) file</h4><ul><li><strong>actions/actions.py file</strong> - The actions.py file is where you define custom actions for your Rasa chatbot. Custom actions are Python functions that can execute arbitrary logic, here we will query LanceDB database and then call OpenAI api for refined/personalized response.</li></ul><p>Let&rsquo;s breakdown and see how to implement actions.py</p><ul><li><p><strong>Import necessary libraries and functions</strong></p></li><li><p><strong>Loading Environment Variables:</strong> Utilizes <code>python-dotenv</code> to securely load sensitive information such as the OpenAI API key from a <code>.env</code> file.</p><p>from typing import Any, Text, Dict, List
from rasa_sdk import Action, Tracker
from rasa_sdk.executor import CollectingDispatcher
import lancedb
import logging
from google.colab import userdata
import openai
import os
from dotenv import load_dotenv</p><h1 id=load-environment-variables-from-env>Load environment variables from .env</h1><p>load_dotenv()</p><h1 id=configure-logging>Configure logging</h1><p>logger = logging.getLogger(<strong>name</strong>)
logging.basicConfig(level=logging.INFO)</p><p>class ActionSearchKnowledge(Action):
def name(self) -> Text:
return &ldquo;action_search_knowledge&rdquo;</p></li></ul><p>We will implement 3 functions in this class</p><ul><li><p><strong>Initialization (<code>__init__</code>):</strong></p></li><li><p>Sets up the OpenAI API key.</p></li><li><p>Establishes a connection to LanceDB and accesses the <code>knowledge_base</code> table.</p><pre><code>  def __init__(self):

      # Initialize OpenAI API key from environment variables
      self.openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
      if not self.openai_api_key:
          logger.error(&quot;OpenAI API key not found. Please set OPENAI_API_KEY in your environment.&quot;)
      openai.api_key = self.openai_api_key

      # Initialize LanceDB connection once
      try:
          self.db = lancedb.connect(&quot;./content/lancedb&quot;)
          self.table_name = &quot;knowledge_base&quot;
          if self.table_name not in self.db.table_names():
              logger.error(f&quot;Table '{self.table_name}' does not exist in LanceDB.&quot;)
              self.table = None
          else:
              self.table = self.db.open_table(self.table_name)
              logger.info(f&quot;Connected to table '{self.table_name}' in LanceDB.&quot;)
      except Exception as e:
          logger.error(f&quot;Error connecting to LanceDB: {e}&quot;)
          self.table = None
</code></pre></li><li><p><strong>Run Method (<code>run</code>):</strong></p></li><li><p><strong>Get User Message:</strong> Retrieve user message from the tracker.</p></li><li><p><strong>Knowledge Retrieval:</strong> Performs a semantic search in LanceDB to find the most relevant piece of information based on the user&rsquo;s query.</p></li><li><p><strong>Response Generation:</strong> Sends the retrieved information to generate_response(&mldr;) or gives direct response decided by relevance of the user&rsquo;s query on LanceDB table.</p></li><li><p><strong>Error Handling:</strong> Gracefully manages any errors by informing the user and logging the issue for further investigation.</p><pre><code>def run(self, dispatcher: CollectingDispatcher,
      tracker: Tracker,
      domain: Dict[Text, Any]) -&gt; List[Dict[Text, Any]]:


  # Get the latest user message
  user_message = tracker.latest_message.get('text')
  logger.info(f&quot;User message: {user_message}&quot;)

  if not user_message:
      dispatcher.utter_message(text=&quot;Sorry, I didn't catch that. Could you please repeat?&quot;)
      return []

  try:
      # Perform similarity search in LanceDB
      query_result = self.table.search(user_message).limit(1).to_pandas()

      # Filter results based on the _distance parameter (smaller _distance means more similar)
      relevant_content = [query_result.loc[0, &quot;content&quot;] if query_result.loc[0, &quot;_distance&quot;] &lt; 0.65 else None][0]
      response_text = &quot;Null&quot;

      # If we find relevant content , sent it to LLM or Else send automated reply
      if not relevant_content == None:
          logger.info(f&quot;Retrieved answer from knowledge base.&quot;)
          # Use OpenAI to generate a more refined response
          response_text = self.generate_response(user_message, relevant_content)
      else:
          # If user has ask not a relevant question, reply with the following
          response_text = &quot;I'm sorry, I don't have an answer to that question.&quot;
          logger.info(f&quot;No matching content found in knowledge base.&quot;)

      # Send the answer back to the user
      dispatcher.utter_message(text=response_text)

  except Exception as e:
      logger.error(f&quot;Error during search operation: {e}&quot;)
      dispatcher.utter_message(text=&quot;Sorry, something went wrong while processing your request.&quot;)

  return []
</code></pre></li><li><p><strong><code>generate_response</code> Method:</strong></p></li><li><p>Constructs a prompt combining the user&rsquo;s question and the relevant knowledge base content.</p></li><li><p>Calls OpenAI&rsquo;s API to generate a refined response.</p></li><li><p>Implements a fallback mechanism in case of API failures.</p><pre><code>def generate_response(self, user_message: Text, relevant_content: Text) -&gt; Text:
  &quot;&quot;&quot;
  Use OpenAI's API to generate a refined response based on user message and relevant content.
  &quot;&quot;&quot;
  try:
      system_prompt = &quot;You are an company support assistant that provides helpful and accurate answers based on the provided information. You talk professionally and like a customer support executive.&quot;

      prompt = (
          f&quot;User Question: {user_message}\n&quot;
          f&quot;Relevant Information: {relevant_content}\n\n&quot;
          f&quot;Provide a detailed and helpful response to the user's question based on the relevant information above.&quot;
      )

      response = openai.ChatCompletion.create(
          model=&quot;gpt-4o-mini&quot;,
          messages=[
              {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt},
              {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
          ],
          max_tokens=450,
          temperature=0.7,
      )

      generated_text = response.choices[0].message['content'].strip()
      logger.info(&quot;Generated response using OpenAI API.&quot;)
      return generated_text

  except Exception as e:
      logger.error(f&quot;Error generating response with OpenAI API: {e}&quot;)
      return relevant_content  # Fallback to relevant content if OpenAI fails
</code></pre></li></ul><h4 id=step-4---training-rasa-model>Step 4 - Training RASA Model</h4><p>After setting up the configuration and integrating necessary components, the next step is to train the Rasa model.</p><pre><code>rasa train
</code></pre><ul><li>The training process compiles the <code>nlu.yml</code>, <code>stories.yml</code>, and other configuration files to create a model that can interpret user intents, extract entities, and manage dialogue flows.</li><li>A confirmation message indicates the successful creation of the model, typically stored in the <code>models</code> directory.</li></ul><h4 id=step-5---run-rasa-server-and-action-server>Step 5 - Run Rasa Server and Action Server</h4><p>To operationalize the chatbot, both the <strong>Rasa Server</strong> and the <strong>Action Server</strong> must be running concurrently. In environments like <strong>Google Colab</strong>, where running multiple persistent processes is challenging, leveraging Python&rsquo;s threading capabilities facilitates simultaneous server execution.</p><pre><code>rasa run
rasa run actions
</code></pre><p>Action server runs on port <code>5055</code> while the Rasa server run on port <code>5005</code>. Make sure they are free otherwise error might come.</p><h3 id=step-6---interact-with-the-bot-and-ask-it-questions>Step 6 - Interact with the bot and ask it questions</h3><pre><code># Function to send messages to the Rasa server
def send_message(message):
    url = &quot;http://localhost:5005/webhooks/rest/webhook&quot;
    payload = {
        &quot;sender&quot;: &quot;test_user&quot;,
        &quot;message&quot;: message
    }
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;
    }
    try:
        response = requests.post(url, data=json.dumps(payload), headers=headers)
        return response.json()
    except requests.exceptions.ConnectionError:
        return {&quot;error&quot;: &quot;Could not connect to Rasa server.&quot;}

# Example interactions
print(&quot;User: Hi&quot;)
assistant_response = send_message(&quot;Hi&quot;)
if assistant_response:
    for resp in assistant_response:
        if isinstance(resp, dict) and &quot;text&quot; in resp:
            print(&quot;Assistant:&quot;, resp[&quot;text&quot;])
        else:
            print(&quot;Assistant:&quot;, resp)

print(&quot;\nUser: How do I reset my password? Explain in french&quot;)
assistant_response = send_message(&quot;How do I delete my account? Explain in french&quot;)
if assistant_response:
    print(&quot;Assistant:&quot;, end = &quot; &quot;)
    for resp in assistant_response:
        if isinstance(resp, dict) and &quot;text&quot; in resp:
            print(resp[&quot;text&quot;])
        else:
            print(resp)
</code></pre><h2 id=colab-walkthrough>Colab Walkthrough</h2><p>[</p><p>Google Colab</p><p><img src=__GHOST_URL__/content/images/icon/favicon-18.ico alt></p><p><img src=__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-18.png alt>
](<a href=https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/RASA_Customer-support-bot/main.ipynb>https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/RASA_Customer-support-bot/main.ipynb</a>)</p><h2 id=conclusion>Conclusion</h2><p>Congratulations! 🎉 You&rsquo;ve just navigated through the exciting process of building an <strong>Advanced Customer Support Chatbot</strong> using <strong>Rasa</strong>, <strong>LanceDB</strong>, and <strong>OpenAI&rsquo;s LLM</strong>. By integrating these powerful tools, you&rsquo;ve created a chatbot that delivers accurate, timely, and personalized support to the customer.</p></div></article><div class=author-section><img src=/assets/authors/default-avatar.png alt="Rithik Kumar" class=author-avatar><div class=author-info><h3 class=author-name>Rithik Kumar</h3><div class=author-social></div></div></div><div class=related-posts><h2>Related Posts</h2><div class=related-posts-grid><article class=related-post><a href=/blog/columnar-file-readers-in-depth-repetition-definition-levels/><img src=/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/columnar-file-readers-in-depth-repetition-definition-levels.png alt="Columnar File Readers in Depth: Repetition & Definition Levels" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-repetition-definition-levels/>Columnar File Readers in Depth: Repetition & Definition Levels</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>June 2, 2025</span></div></article><article class=related-post><a href=/blog/columnar-file-readers-in-depth-column-shredding/><img src=/assets/blog/columnar-file-readers-in-depth-column-shredding/columnar-file-readers-in-depth-column-shredding.png alt="Columnar File Readers in Depth: Column Shredding" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-column-shredding/>Columnar File Readers in Depth: Column Shredding</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>May 15, 2025</span></div></article><article class=related-post><a href=/blog/columnar-file-readers-in-depth-compression-transparency/><img src=/assets/blog/columnar-file-readers-in-depth-compression-transparency/columnar-file-readers-in-depth-compression-transparency.png alt="Columnar File Readers in Depth: Compression Transparency" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-compression-transparency/>Columnar File Readers in Depth: Compression Transparency</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>April 29, 2025</span></div></article></div></div></main></div><footer class=site-footer><div class=footer-content><a href=/ class=site-title><img src=/assets/blog/logo.png alt="LanceDB Blog" class=site-logo></a><div class=footer-links><a href=https://lancedb.com/documentation class=footer-link>Documentation</a>
<a href=https://lancedb.com/pricing class=footer-link>Pricing</a>
<a href=/get-started class="footer-link get-started">Get Started</a></div></div></footer></body></html>