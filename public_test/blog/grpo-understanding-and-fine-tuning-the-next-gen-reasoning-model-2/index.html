<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>RAG with GRPO Fine-Tuned Reasoning Model - LanceDB Blog</title><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/syntax-highlighting.css><script src=/js/toc-progress.js></script><script src=/js/heading-links.js></script><script src=/js/announcement.js></script><script src=/js/code-blocks.js></script></head><body><div class=announcement-bar id=announcement-bar><div class=announcement-content><span>June 1st, 2025: LanceDB Cloud is now in public beta!</span>
<a href=https://lancedb.com/cloud class=announcement-link>Try it now ‚Üí</a>
<button class=announcement-close onclick=closeAnnouncement()>√ó</button></div></div><header class=site-header><div class=header-content><a href=/ class=site-title><img src=/assets/blog/logo.png alt="LanceDB Blog" class=site-logo></a><div class=header-links><a href=/docs class=header-link>Documentation</a>
<a href=/pricing class=header-link>Pricing</a>
<a href=/get-started class="header-link get-started">Get Started</a></div></div></header><div class=content-wrapper><div class=toc-container><div class=toc><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><ul><li><a href=#how-does-it-work-roughly>How does it work, roughly?</a></li><li><a href=#components-of-grpo>Components of GRPO</a></li><li><a href=#now-lets-fine-tune-some-model-on-grpo-for-token-diversity-and-length-constraints>Now let&rsquo;s fine tune some model on GRPO for Token Diversity and Length Constraints</a></li><li><a href=#model-inference>Model Inference</a></li><li><a href=#using-the-model-as-a-rag-with-lancedb>Using the model as a RAG with LanceDB</a></li></ul></li></ul></nav></div></div><main><h1>RAG with GRPO Fine-Tuned Reasoning Model</h1><div class=post-meta><span class=post-category>Engineering</span>
<span class=meta-divider>‚Ä¢</span>
<span class=post-author>Mahesh Deshwal</span>
<span class=meta-divider>‚Ä¢</span>
<span class=post-date>March 24, 2025</span></div><article class=single-post><img src=/assets/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2.png alt="RAG with GRPO Fine-Tuned Reasoning Model" class=preview-image><div class=post-content><div class="admonition info"><div class=admonition-content><div class=admonition-title>üí° Community Post</div>This is a community post by Mahesh Deshwal</div></div><p><strong>Group Relative Policy Optimization</strong> is the series of RL techniques for LLMs to guide them to specific goals. The process of creating a smart model these days is something like this:</p><ol><li>Pre Training a model on a HUGE corpus to get a pre-trained model</li><li>Use the model created above on your data (usually Instructions or Que-Ans format) to fine-tune in an SFT (Supervised fine-tuning) manner.</li></ol><p>Now let&rsquo;s say a question can be answered in 100 different ways by the above model, all of them are correct. For example, the answer can be one word, sarcastic, politically incorrect, harmful, etc. If you have to <strong>GUIDE</strong> the model to a specific set of rules for generation, you use the RL (Reinforcement Learning), where the popular techniques are PPO and DPO, and with DeepSeek, there comes GRPO.</p><h3 id=how-does-it-work-roughly>How does it work, roughly?</h3><p>While using PPO or DPO, you don&rsquo;t have a ground truth label for the data. You generate more than 1 answer to a question. You then use a dedicated model (generally an LLM) that scores the answers and then based on the scores, you choose which answer is good. So it is basically selecting and rejecting similar answers based on some criteria.</p><p><img src=/assets/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/image_fx_.jpg.png alt="GRPO Process Overview"></p><p>But in GRPO, there is no Critique or Scorer. What they do is simply decide multiple rules (python functions) to give scores to each answer. Then generate <code>G</code> responses to a question, calculate all the scores, and convert them to one single weighted score which is the <code>Normalized Z score</code>. Then given if an answer&rsquo;s score is more or less than that Z score, they accept or reject. Let&rsquo;s understand each and every component of the GRPO equation with intuition and line-by-line code.</p><h3 id=components-of-grpo>Components of GRPO</h3><p><img src=/assets/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/Screenshot-2025-03-06-at-1.30.07-AM--1--1.png alt="GRPO Components"></p><p>Code snippets below are referred from <a href=https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py>HuggingFace <code>GRPOTrainer</code></a></p><ul><li><p><strong>Overall Objective (ùí•GRPO(Œ∏))</strong>: This is the GRPO Loss function we need to improve our model. Each components of this loss are defined below.</p></li><li><p><strong>Question Sampling (q ‚àº P(Q))</strong>: A question (AKA PROMPT) is randomly chosen from a pool of questions. This represents the problem or prompt that the model needs to answer.</p></li><li><p><strong>Group of Outputs ({oi} with G outputs)</strong>: For each question, we generate several answers (G in total, GRPOTrainer uses 8 by default) using the model from the previous training step. These outputs are like multiple attempts at answering the same question. This is the Crux of the equation because we don&rsquo;t have a Ground truth so we rate THESE answers based on some pre defined function and compute the score for EACH of the Answer and then see if an answer is above or below Average score of group</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>sampling_params</span> <span class=o>=</span> <span class=n>SamplingParams</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>num_generations</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><strong>Reference Policy (œÄref)</strong>: This is the model that we fine-tuned using SFT. It serves as a base to ensure that the model&rsquo;s updates do start giving weird answers.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=bp>self</span><span class=o>.</span><span class=n>ref_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=o>**</span><span class=n>model_init_kwargs</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><strong>Current Policy (œÄŒ∏)</strong>: This is the model we are actively training and updating. During output generation, we compare how the current policy behaves relative to the old policy &ldquo;LOGIT by LOGIT"Loss is computed Logit vise</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=o>**</span><span class=n>model_init_kwargs</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><p><strong>Old Policy (œÄŒ∏_old)</strong>: Same model as above BUT this is the version of from the previous epoch. It also has &ldquo;G&rdquo; answers to the Prompts but for the &ldquo;PREVIOUS&rdquo; epoch</p></li><li><p><strong>Token Advantage (ƒ§Ai,t)</strong>: This term represents the normalized reward (or advantage) associated with &ldquo;EACH TOKEN&rdquo;. It indicates how much better or worse the generation related to Z score. One thing is that score is Given for final Generation BUT it is replicated as same for EACH Logit in the answer</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>advantages</span> <span class=o>=</span> <span class=p>(</span><span class=n>rewards</span> <span class=o>-</span> <span class=n>mean_grouped_rewards</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>std_grouped_rewards</span> <span class=o>+</span> <span class=mf>1e-4</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><strong>Clipping Range (1 ‚Äì Œµ to 1 + Œµ)</strong>: The ratio between the current and old policies is clipped within this range to prevent large, unstable updates. This ensures that each update is moderate and controlled.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>coef_1</span><span class=p>,</span> <span class=mi>1</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span><span class=p>,</span> <span class=mi>1</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>epsilon</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><strong>Scaling Factor (Œ≤)</strong>: This factor scales the penalty from the KL divergence. It controls how strongly the model is regularized.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>per_token_loss</span> <span class=o>=</span> <span class=n>per_token_loss</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>per_token_kl</span>
</span></span></code></pre></div><ul><li><strong>KL Divergence Penalty (ùîªKL[œÄŒ∏ || œÄref])</strong>: This term measures the difference between the current policy and the reference policy. By penalizing large differences, it helps keep the updated model close to the original fine-tuned model, ensuring that improvements are made without losing previously learned skills. It is also implemented <strong>PER TOKEN</strong></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>per_token_kl</span> <span class=o>=</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>ref_per_token_logps</span> <span class=o>-</span> <span class=n>per_token_logps</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span><span class=n>ref_per_token_logps</span> <span class=o>-</span> <span class=n>per_token_logps</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=now-lets-fine-tune-some-model-on-grpo-for-token-diversity-and-length-constraints>Now let&rsquo;s fine tune some model on GRPO for Token Diversity and Length Constraints</h3><p>You can use any model and tool of your choice but we&rsquo;ll use <code>PEFT: LoRA</code> using <code>HuggingFace TRL</code> to do it all on 1 GPU on Colab</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>qqq</span> <span class=n>unsloth</span> <span class=n>vllm</span> <span class=o>--</span><span class=n>progress</span><span class=o>-</span><span class=n>bar</span> <span class=n>off</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span><span class=p>,</span> <span class=n>Dataset</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>GRPOConfig</span><span class=p>,</span> <span class=n>GRPOTrainer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>get_peft_model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>max_seq_length</span> <span class=o>=</span> <span class=mi>512</span> <span class=c1># Can increase for longer reasoning traces</span>
</span></span><span class=line><span class=cl><span class=n>lora_rank</span> <span class=o>=</span> <span class=mi>8</span> <span class=c1># Larger rank = smarter, but slower</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load model</span>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;HuggingFaceTB/SmolLM-135M-Instruct&#34;</span> <span class=c1># VERY Small model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load LoRA</span>
</span></span><span class=line><span class=cl><span class=n>lora_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>task_type</span><span class=o>=</span><span class=s2>&#34;CAUSAL_LM&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>r</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>target_modules</span><span class=o>=</span><span class=s2>&#34;all-linear&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>lora_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>print_trainable_parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load dataset</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;mlabonne/smoltldr&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=reward-functions>Reward Functions</h4><p>Let&rsquo;s define some Reward functions. These will be applied to Each and Every generation and then used as weighted sum (default to 1 for each) to get advantage. You an use ANY reward function based on your choice but I&rsquo;m just writing generic ones here</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>reward_len</span><span class=p>(</span><span class=n>completions</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Function to give rewards based on Length of the Answer&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=o>-</span><span class=nb>abs</span><span class=p>(</span><span class=mi>50</span> <span class=o>-</span> <span class=nb>len</span><span class=p>(</span><span class=n>completion</span><span class=p>))</span> <span class=k>for</span> <span class=n>completion</span> <span class=ow>in</span> <span class=n>completions</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>reward_token_diversity</span><span class=p>(</span><span class=n>completions</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Rewards completions with a higher ratio of unique tokens&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>completion</span> <span class=ow>in</span> <span class=n>completions</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>tokens</span> <span class=o>=</span> <span class=n>completion</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>tokens</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>diversity</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>tokens</span><span class=p>))</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>rewards</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>diversity</span> <span class=o>*</span> <span class=mi>100</span><span class=p>)</span>  <span class=c1># scaling factor for reward</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>rewards</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>rewards</span>
</span></span></code></pre></div><p>Now just off to training the model as usual with very few modifications</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>max_prompt_length</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>training_args</span> <span class=o>=</span> <span class=n>GRPOConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>5e-6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>adam_beta1</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>adam_beta2</span> <span class=o>=</span> <span class=mf>0.99</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>warmup_ratio</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lr_scheduler_type</span> <span class=o>=</span> <span class=s2>&#34;cosine&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>optim</span> <span class=o>=</span> <span class=s2>&#34;paged_adamw_8bit&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logging_steps</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span> <span class=o>=</span> <span class=mi>6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient_accumulation_steps</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=c1># Increase to 4 for smoother training</span>
</span></span><span class=line><span class=cl>    <span class=n>num_generations</span> <span class=o>=</span> <span class=mi>6</span><span class=p>,</span> <span class=c1># Decrease if out of memory</span>
</span></span><span class=line><span class=cl>    <span class=n>max_prompt_length</span> <span class=o>=</span> <span class=n>max_prompt_length</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_completion_length</span> <span class=o>=</span> <span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># num_train_epochs = 1, # Set to 1 for a full training run</span>
</span></span><span class=line><span class=cl>    <span class=n>max_steps</span> <span class=o>=</span> <span class=mi>250</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>save_steps</span> <span class=o>=</span> <span class=mi>250</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_grad_norm</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>report_to</span> <span class=o>=</span> <span class=s2>&#34;none&#34;</span><span class=p>,</span> <span class=c1># Can use Weights &amp; Biases</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span> <span class=o>=</span> <span class=s2>&#34;outputs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>GRPOTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>processing_class</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>reward_funcs</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>reward_len</span><span class=p>,</span> <span class=n>reward_token_diversity</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span> <span class=o>=</span> <span class=n>training_args</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>[</span><span class=s2>&#34;train&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=model-inference>Model Inference</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2># About the GOAT:
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>Lionel Andr√©s &#34;Leo&#34; Messi[note 1] (Spanish pronunciation: [ljoÀànel anÀàd…æes Ààmesi] ‚ìò; born 24 June 1987) is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team. Widely regarded as the greatest player of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d&#39;Or awards and four the Best FIFA Men&#39;s Player awards.[note 2] He is the most decorated player in the history of professional football having won 45 team trophies,[note 3] including twelve league titles, four UEFA Champions Leagues, two Copa Am√©ricas, and one FIFA World Cup. Messi holds the records for most European Golden Shoes (6), most goals in a calendar year (91), most goals for a single club (672, with Barcelona), most goals (474), hat-tricks (36) and assists (192) in La Liga, most assists (18) and goal contributions (32) in the Copa Am√©rica, most goal contributions (21) in the World Cup, most international appearances (191) and international goals (112) by a South American male, and the second-most in the latter category outright. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country.[16]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. He gradually established himself as an integral player for the club, and during his first uninterrupted season at age 22 in 2008‚Äì09 he helped Barcelona achieve the first treble in Spanish football. This resulted in Messi winning the first of four consecutive Ballons d&#39;Or, and by the 2011‚Äì12 season he would set La Liga and European records for most goals in a season and establish himself as Barcelona&#39;s all-time top scorer. The following two seasons, he finished second for the Ballon d&#39;Or behind Cristiano Ronaldo, his perceived career rival. However, he regained his best form during the 2014‚Äì15 campaign, where he became the all-time top scorer in La Liga, led Barcelona to a historic second treble, and won a fifth Ballon d&#39;Or in 2015. He assumed Barcelona&#39;s captaincy in 2018 and won a record sixth Ballon d&#39;Or in 2019. During his overall tenure at Barcelona, Messi won a club-record 34 trophies, including ten La Liga titles and four Champions Leagues, among others. Financial difficulties at Barcelona led to Messi signing with French club Paris Saint-Germain in August 2021, where he would win the Ligue 1 title during both of his seasons there. He joined Major League Soccer club Inter Miami in July 2023.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>An Argentine international, Messi is the national team&#39;s all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor. At the youth level, he won the 2005 FIFA World Youth Championship and gold medal in the 2008 Summer Olympics. After his senior debut in 2005, Messi became the youngest Argentine to play and score in a World Cup in 2006. Assuming captaincy in 2011, he then led Argentina to three consecutive finals in the 2014 FIFA World Cup, the 2015 Copa Am√©rica and the Copa Am√©rica Centenario, all of which they would lose. After initially announcing his international retirement in 2016, he returned to help his country narrowly qualify for the 2018 FIFA World Cup, which they would exit early. Messi and the national team finally broke Argentina&#39;s 28-year trophy drought by winning the 2021 Copa Am√©rica, which helped him secure his seventh Ballon d&#39;Or that year. He then led Argentina to win the 2022 Finalissima, as well as the 2022 FIFA World Cup, his country&#39;s third overall world championship and first in 36 years. This followed with a record-extending eighth Ballon d&#39;Or in 2023, and a victory in the 2024 Copa Am√©rica.
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>merged_model</span> <span class=o>=</span> <span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>merge_and_unload</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>generator</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;text-generation&#34;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span> <span class=n>merged_model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_text</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>generated_text</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=using-the-model-as-a-rag-with-lancedb>Using the model as a RAG with LanceDB</h3><p>Smooth as a üç∞ üö∂.</p><p>Let&rsquo;s take a look at some interesting use cases. You are a screenplay writer and you have an idea about a movie but don&rsquo;t want the cliche scenario. Maybe you don&rsquo;t care about Older movies so you fetch the movies from the last 30 years or so using <code>PREFILTER</code>. Given we had lexical diversity AND length based rewards in GRPO, we&rsquo;re now not only creative in words but always to the point too. So let&rsquo;s test our ideas of &ldquo;what not to write about&rdquo; as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>lancedb</span> <span class=n>sentence</span><span class=o>-</span><span class=n>transformers</span> <span class=o>-</span><span class=n>qqq</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>lancedb</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lancedb.pydantic</span> <span class=kn>import</span> <span class=n>LanceModel</span><span class=p>,</span> <span class=n>Vector</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lancedb.embeddings</span> <span class=kn>import</span> <span class=n>get_registry</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;hf://datasets/vishnupriyavr/wiki-movie-plots-with-summaries/wiki_movie_plots_deduped_with_summaries.csv&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>rename</span><span class=p>(</span><span class=n>columns</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;Release Year&#34;</span><span class=p>:</span> <span class=s2>&#34;release_year&#34;</span><span class=p>,</span> <span class=s2>&#34;Title&#34;</span><span class=p>:</span> <span class=s2>&#34;title&#34;</span><span class=p>,</span> <span class=s2>&#34;Plot&#34;</span><span class=p>:</span> <span class=s2>&#34;movie_plot&#34;</span><span class=p>},</span>  <span class=n>inplace</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>loc</span><span class=p>[:,</span> <span class=p>[</span><span class=s2>&#34;release_year&#34;</span><span class=p>,</span> <span class=s2>&#34;title&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_plot&#34;</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>db</span> <span class=o>=</span> <span class=n>lancedb</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=s2>&#34;/tmp/db&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>get_registry</span><span class=p>()</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;sentence-transformers&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>create</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s2>&#34;BAAI/bge-small-en-v1.5&#34;</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TableSchema</span><span class=p>(</span><span class=n>LanceModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>movie_plot</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>SourceField</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>vector</span><span class=p>:</span> <span class=n>Vector</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>ndims</span><span class=p>())</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>VectorField</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>title</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>    <span class=n>release_year</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>table</span> <span class=o>=</span> <span class=n>db</span><span class=o>.</span><span class=n>create_table</span><span class=p>(</span><span class=s2>&#34;movie_plots&#34;</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=n>TableSchema</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>table</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=n>df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;What&#39;s common in movies where the protagonist turns out to be the bad guy&#34;</span>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=s2>&#34;</span><span class=se>\n\n</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=sa>f</span><span class=s2>&#34;## Movie- </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>:</span><span class=se>\n</span><span class=si>{</span><span class=n>item</span><span class=p>[</span><span class=s1>&#39;movie_plot&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span><span class=n>item</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>table</span><span class=o>.</span><span class=n>search</span><span class=p>(</span><span class=n>query</span><span class=p>)</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;release_year &lt; </span><span class=si>{</span><span class=mi>1990</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>limit</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>to_list</span><span class=p>())])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># NOW you pass this Query + Context to the model and get to the point results</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;Given the Query: </span><span class=si>{</span><span class=n>query</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>And the context</span><span class=se>\n</span><span class=s2>#Context</span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=se>\n\n</span><span class=s2>Answer concisely but creatively&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_text</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>generated_text</span><span class=p>)</span>
</span></span></code></pre></div><p>Would really love to know what your result was for this query with your fine-tuned model üòÑ</p></div></article><div class=author-section><img src=/assets/authors/community.jpg alt="Mahesh Deshwal" class=author-avatar><div class=author-info><h3 class=author-name>Mahesh Deshwal</h3><p class=author-bio>ML Engineer and researcher specializing in reinforcement learning, fine-tuning techniques, and practical AI applications.</p><div class=author-social></div></div></div><div class=related-posts><h2>Related Posts</h2><div class=related-posts-grid><article class=related-post><a href=/blog/columnar-file-readers-in-depth-repetition-definition-levels/><img src=/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/columnar-file-readers-in-depth-repetition-definition-levels.png alt="Columnar File Readers in Depth: Repetition & Definition Levels" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-repetition-definition-levels/>Columnar File Readers in Depth: Repetition & Definition Levels</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>June 2, 2025</span></div></article><article class=related-post><a href=/blog/columnar-file-readers-in-depth-column-shredding/><img src=/assets/blog/columnar-file-readers-in-depth-column-shredding/columnar-file-readers-in-depth-column-shredding.png alt="Columnar File Readers in Depth: Column Shredding" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-column-shredding/>Columnar File Readers in Depth: Column Shredding</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>May 15, 2025</span></div></article><article class=related-post><a href=/blog/columnar-file-readers-in-depth-compression-transparency/><img src=/assets/blog/columnar-file-readers-in-depth-compression-transparency/columnar-file-readers-in-depth-compression-transparency.png alt="Columnar File Readers in Depth: Compression Transparency" class=related-preview-image></a><h3><a href=/blog/columnar-file-readers-in-depth-compression-transparency/>Columnar File Readers in Depth: Compression Transparency</a></h3><div class=post-meta><span class=post-author>Weston Pace</span>
<span class=post-date>April 29, 2025</span></div></article></div></div></main></div><footer class=site-footer><div class=footer-content><a href=/ class=site-title><img src=/assets/blog/logo.png alt="LanceDB Blog" class=site-logo></a><div class=footer-links><a href=https://lancedb.com/documentation class=footer-link>Documentation</a>
<a href=https://lancedb.com/pricing class=footer-link>Pricing</a>
<a href=/get-started class="footer-link get-started">Get Started</a></div></div></footer></body></html>