

import config
import fetch_docs
import embed_ingest
from datetime import datetime, timedelta
import textwrap
import pandas as pd
import os
import shutil
from typing import List, Dict, Any, Optional
import lancedb

def find_next_publication_date(start_date_str: str, max_days_to_search: int = 7) -> (str, list):
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
    for i in range(1, max_days_to_search + 1):
        check_date = start_date + timedelta(days=i)
        date_str = check_date.strftime('%Y-%m-%d')
        docs = fetch_docs.fetch_documents(date_str, per_page=500)
        if docs:
            return date_str, docs
    return None, []

def print_search_result(result_df: pd.DataFrame, version: str):
    print(f"--- Top Result for Version: {version} ---")
    if result_df.empty:
        print("No results found for this version.")
        return

    top_result = result_df.iloc[0]
    print(f"üìÑ Title: {top_result['title']}")
    print(f"üóìÔ∏è  Date: {top_result['publication_date']}")
    print(f"üìè Distance: {top_result['_distance']:.4f}")
    
    abstract = top_result.get('abstract') or "[No abstract available for this document]"
    
    print(f"üìù Abstract:{textwrap.fill(abstract, width=100)}")


def run_workflow():
    print("--- Initializing Database Environment ---")
    if os.path.exists(config.DB_URI):
        shutil.rmtree(config.DB_URI)
        print(f"Removed old database at {config.DB_URI}")
    os.makedirs(config.DB_URI, exist_ok=True)
    db = lancedb.connect(config.DB_URI)
    
    prod_model = embed_ingest.get_embedding_model(config.PRODUCTION_MODEL)

    print("\n--- STEP 1: Initial Data Ingestion ---")
    initial_docs = []
    last_publication_date = config.REPRODUCIBLE_START_DATE
    
    last_publication_date, initial_docs = find_next_publication_date(last_publication_date)
    if not initial_docs:
        print("Failed to fetch any data to create an initial table. Exiting.")
        return
    
    initial_data = embed_ingest.embed_documents(prod_model, initial_docs)
    
    print(f"\nCreating table '{config.TABLE_NAME}'...")
    tbl = db.create_table(config.TABLE_NAME, data=initial_data)
    print(f"‚úÖ Table '{config.TABLE_NAME}' created. Version: {tbl.version}, Rows: {len(tbl)}")

    print("\n--- STEP 2: Simulating Sequential Daily Updates ---")
    
    last_publication_date, update_docs_1 = find_next_publication_date(last_publication_date)
    if update_docs_1:
        update_data_1 = embed_ingest.embed_documents(prod_model, update_docs_1)
        tbl.add(update_data_1)
        print(f"‚úÖ Data added to '{tbl.name}'. New Version: {tbl.version}, Total Rows: {len(tbl)}")

    last_publication_date, update_docs_2 = find_next_publication_date(last_publication_date)
    if update_docs_2:
        update_data_2 = embed_ingest.embed_documents(prod_model, update_docs_2)
        tbl.add(update_data_2)
        print(f"‚úÖ Data added to '{tbl.name}'. New Version: {tbl.version}, Total Rows: {len(tbl)}")
        
    print("\n========================================================")
    print("= PART 1: AUDITING KNOWLEDGE BASE ACROSS TIME  =")
    print("========================================================")
    
    query_text = "cybersecurity reporting requirements for public companies"
    print(f"\nRunning audit for query: '{query_text}'")
    query_vector = prod_model.encode(query_text)

    for version_num in range(1, tbl.version + 1):
        tbl_to_search = db.open_table(config.TABLE_NAME)
        tbl_to_search.checkout(version_num)
        print(f"‚úÖ Successfully checked out Version {version_num} of '{config.TABLE_NAME}'. Total rows: {len(tbl_to_search)}")
        
        if tbl_to_search:
            print(f"\nQuerying table '{tbl_to_search.name}' (Version {tbl_to_search.version})...")
            search_results = tbl_to_search.search(query_vector).limit(1).to_pandas()
            print_search_result(search_results, f"V{version_num} ({config.PRODUCTION_MODEL})")

    print("\n‚úÖ Date-based audit complete. Results show how knowledge evolves over time.")

    print("\n=============================================================")
    print("= PART 2: A/B TESTING DIFFERENT EMBEDDING MODELS  =")
    print("=============================================================")
    
    exp_model = embed_ingest.get_embedding_model(config.EXPERIMENTAL_MODEL)
    
    db.drop_table(config.EXPERIMENTAL_TABLE_NAME, ignore_missing=True)
        
    print(f"\nCreating table '{config.EXPERIMENTAL_TABLE_NAME}'...")
    exp_tbl = db.create_table(
        config.EXPERIMENTAL_TABLE_NAME,
        data=embed_ingest.embed_documents(exp_model, tbl.to_pandas().to_dict('records'))
    )
    print(f"‚úÖ Table '{config.EXPERIMENTAL_TABLE_NAME}' created. Version: {exp_tbl.version}, Rows: {len(exp_tbl)}")
    
    if exp_tbl:
        print("\nComparing search results for the same data with different models:")
        
        print(f"\nQuerying table '{tbl.name}' (Version {tbl.version})...")
        print_search_result(
            tbl.search(query_vector).limit(1).to_pandas(),
            f"Latest Prod V{tbl.version} ({config.PRODUCTION_MODEL})"
        )
        
        print(f"\nQuerying table '{exp_tbl.name}' (Version {exp_tbl.version})...")
        print_search_result(
            exp_tbl.search(exp_model.encode(query_text)).limit(1).to_pandas(),
            f"Experimental ({config.EXPERIMENTAL_MODEL})"
        )

    print("\n‚úÖ A/B test complete. Notice the difference in relevance (distance score) between models.")


if __name__ == "__main__":
    run_workflow()
