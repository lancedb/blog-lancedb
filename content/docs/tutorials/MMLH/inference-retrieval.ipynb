{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Inference and Retrieval with LanceDB\n",
    "\n",
    "Welcome to the second part of our tutorial on building an advanced product search engine. In the first part, we focused on feature engineering, enriching our dataset with a variety of features. Now, we'll build the inference and retrieval pipeline that uses these features to provide a powerful and intuitive search experience.\n",
    "\n",
    "We will cover the following steps:\n",
    "1. **Setting up the environment**: Installing the necessary libraries and loading our trained models.\n",
    "2. **Query Routing**: Using a Large Language Model (LLM) to route user queries to the most appropriate feature.\n",
    "3. **Hybrid Search**: Combining vector search and full-text search to get the best of both worlds.\n",
    "4. **Reranking**: Using a reranker to improve the relevance of our search results.\n",
    "5. **Putting it all together**: Building a complete search function that incorporates all these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade geneva lancedb google-genai kubernetes \"ray[default]\" rerankers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and load our models. We'll be using a CLIP model for embedding images, a BAAI model for embedding text, and a Gemini model for query routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "from google import genai\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from lancedb.rerankers import ColbertReranker\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyA0O1dSJnNqMlLM64yr4uLwy4yVKSoJpyA\"\n",
    "genai_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# === CONNECT TO LANCEDB ===\n",
    "DB_PATH = \"./db\"\n",
    "TABLE_NAME = \"products\"\n",
    "db = lancedb.connect(DB_PATH)\n",
    "tbl = db.open_table(TABLE_NAME)\n",
    "\n",
    "# === CLIP FOR QUERY EMBEDDING ===\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda()\n",
    "proc = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# === BAAI FOR TEXT EMBEDDING ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\").cuda()\n",
    "\n",
    "# === FEATURE DESCRIPTIONS FOR ROUTING ===\n",
    "FEATURE_DESCRIPTIONS = {\n",
    "    \"summary_embedding\": \"semantic intent captured from product summary and/or what this product goes well with, for example a formal shoe with a formal shirt etc\",\n",
    "    \"occasion_embedding\": \"semantic intent captured from occasion description like a a formal shirt for a business meeting\"\n",
    "}\n",
    "VECTOR_FEATURES = set(FEATURE_DESCRIPTIONS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Routing\n",
    "\n",
    "Our search engine has multiple features that we can search against. To provide the best results, we need to choose the most appropriate feature for each user query. We'll use a Large Language Model (LLM) to act as a query router. The LLM will analyze the user's query and choose the feature that is most likely to produce relevant results.\n",
    "\n",
    "We'll define a function `choose_feature` that takes a user query as input and returns the name of the best feature to use for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def choose_feature(query: str) -> str:\n",
    "    options = \"\".join(f\"- `{f}`: {d}\" for f, d in FEATURE_DESCRIPTIONS.items())\n",
    "    prompt = (\n",
    "        f\"Given the user query:{query}\"\n",
    "        f\"Which one of the following features best matches the query intent?\"\n",
    "        f\"Choose exactly one option (only return the feature name as plain string without formatting):{options}\"\n",
    "        )\n",
    "    resp = genai_client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config={\"temperature\": 0.0}\n",
    "    )\n",
    "    return resp.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hybrid Search\n",
    "\n",
    "Hybrid search combines the strengths of vector search and full-text search. Vector search is great for finding semantically similar results, while full-text search is great for finding exact keyword matches. By combining the two, we can get the best of both worlds.\n",
    "\n",
    "We'll also add a color filter to our search function. This will allow users to filter their search results by color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COLOR_LIST = [\"black\",\"white\",\"red\",\"blue\",\"green\",\"yellow\",\"pink\",\"orange\",\"grey\",\"brown\"]\n",
    "def extract_color(query: str) -> Optional[str]:\n",
    "    q = query.lower()\n",
    "    for c in COLOR_LIST:\n",
    "        if re.search(rf'\\b{c}\\b', q):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def embed_query(query: str) -> List[float]:\n",
    "    inputs = tokenizer(\n",
    "        query,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "        pooled = out.last_hidden_state.mean(dim=1)  # meanâ€‘pool CLS tokens\n",
    "    return pooled[0].cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reranking\n",
    "\n",
    "To further improve the relevance of our search results, we'll use a reranker. A reranker is a model that takes the initial search results and reorders them based on a more fine-grained relevance score. We'll use the `ColbertReranker` from LanceDB, which is a powerful reranker based on the ColBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cuda\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n",
      "Linear Dim set to: 128 for downcasting\n",
      "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cuda\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n",
      "Linear Dim set to: 128 for downcasting\n"
     ]
    }
   ],
   "source": [
    "reranker_occasion = ColbertReranker(column=\"occasion\")\n",
    "reranker_summary = ColbertReranker(column=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting It All Together\n",
    "\n",
    "Now, let's put everything together into a single search function. This function will:\n",
    "\n",
    "1.  Take a user query as input.\n",
    "2.  Use our LLM-powered query router to choose the best feature to search against.\n",
    "3.  Perform a hybrid search using the chosen feature.\n",
    "4.  Apply a color filter if a color is detected in the query.\n",
    "5.  Rerank the search results using the appropriate reranker.\n",
    "6.  Return the top k results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search(query: str,\n",
    "           k: int = 5,\n",
    "           color_filter: Optional[str] = None) -> List[dict]:\n",
    "    # Route to feature\n",
    "    feature = choose_feature(query)\n",
    "    print(f\"[Router] Selected feature: {feature}\")\n",
    "\n",
    "    # Extract color filter\n",
    "    color = extract_color(query)\n",
    "    if color:\n",
    "        print(f\"[Filter] Color detected: {color}\")\n",
    "\n",
    "    # Always hybrid search on vector feature\n",
    "    search_input = embed_query(query)\n",
    "    reranker = reranker_summary if feature==\"summary_embedding\" else reranker_occasion\n",
    "    qb = tbl.search(query_type=\"hybrid\", vector_column_name=feature).vector(search_input).text(query).rerank(reranker)\n",
    "           \n",
    "    if color:\n",
    "        print(\"filter :\", color)\n",
    "        qb = qb.where(f\"color_tags like '{color}'\", prefilter=True)\n",
    "\n",
    "    qb = qb.limit(k*2) # overfetch and rerank\n",
    "    return qb.to_list()[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Results\n",
    "\n",
    "Let's create a helper function to display our search results in a nice grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_results(results: List[dict], cols: int = 4):\n",
    "    n = len(results)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = axes.flatten()\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    for i, item in enumerate(results):\n",
    "        img = Image.open(io.BytesIO(item['image_bytes']))\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(item.get('description','')[:15], fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Queries\n",
    "\n",
    "Now, let's try out our search engine with a few example queries and also display the route used for generating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "    \"black jacket\",\n",
    "    \"a formal shirt for business meeting\",\n",
    "    \"A tie that would go well with white formal shirt\",\n",
    "    ]\n",
    "    for q in queries:\n",
    "        print(f\"Query: {q}\")\n",
    "        res = search(q, k=5, color_filter=None)\n",
    "        display_results(res, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This is a demonstration of how LanceDB is well suited for building large scale multi-feature, multi-index retreival systems. We've only chosen a few features and built 2 vector indices with semantic routing. In a real world use case, there can be many more complex features that can be extracted or built on top of these features for more powerful retrieval systems. If there are many indeces among which the base query can routed, you might even consider training a classifier for that. \n",
    "\n",
    "Learn more about LanceDB & Geneva on [LanceDB Documentation]()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
