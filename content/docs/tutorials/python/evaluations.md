---
title: "Evaluation"
sidebar_title: "Evaluation"
description: "Develop evaluation applications to measure performance across various metrics"
weight: 8
---

Evaluation: Assessing Text Performance with Precision 
====================================================================

Evaluation is a comprehensive tool designed to measure the performance of text-based inputs, enabling data-driven optimization and improvement . 

Text Evaluation 101 

Using robust framework for assessing reference and candidate texts across various metrics, ensure that the text outputs are high-quality and meet specific requirements and standards.

| Evaluation | Description |
|:-----------|:------------|
| [Evaluating Prompts with Prompttools](https://github.com/lancedb/vectordb-recipes/blob/main/examples/prompttools-eval-prompts) | Compare, visualize & evaluate embedding functions (incl. OpenAI) across metrics like latency & custom evaluation |
| [Evaluating RAG with RAGAs and GPT-4o](https://github.com/lancedb/vectordb-recipes/blob/main/examples/Evaluating_RAG_with_RAGAs) | Evaluate RAG pipelines with cutting-edge metrics and tools, integrate with CI/CD for continuous performance checks, and generate responses with GPT-4o |
